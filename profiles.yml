# 1) Fill in your GCP project name and desired dataset name.
# 2) These settings are meant for "oauth via gcloud" authentication. You need to follow these
#    instructions:
#    https://docs.getdbt.com/reference/warehouse-setups/bigquery-setup#local-oauth-gcloud-setup
#    You can also check other options for BigQuery authentication in
#    https://docs.getdbt.com/reference/warehouse-setups/bigquery-setup#authentication-methods.

fhir_dbt_analytics:
  target: dev
  outputs:
    dev:
      # Name of your GCP project in which the dataset will be created and BigQuery jobs run.
      # See here for creating a new GCP project:
      # https://cloud.google.com/resource-manager/docs/creating-managing-projects
      project: aou-datawarehouse-sandbox

      # Name of the dataset that dbt creates and writes to.
      # Recommended: dbt_<username>. See here for more details:
      # https://docs.getdbt.com/docs/get-started/connection-profiles#understanding-target-schemas
      dataset: "{{ env_var('USER', 'unknown_user') }}_ppi_001"

      # See https://docs.getdbt.com/docs/get-started/connection-profiles#understanding-threads.
      threads: 4

      # Location of your dbt outputs must be the same as the location of your FHIR sources.
      # See https://docs.getdbt.com/reference/warehouse-setups/bigquery-setup#dataset-locations.
      location: US

      job_execution_timeout_seconds: 300
      job_retries: 1
      method: oauth
      priority: interactive
      type: bigquery
